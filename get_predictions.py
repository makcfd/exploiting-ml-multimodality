import os
import tqdm
from torch import load, cuda, backends, no_grad, nn, cuda
from my_utilities.dataproviderNN import get_data
import pandas as pd

# CONST VAIABLES
DATASET_PATH = "/exploiting_model_multiplicity/data/"
test_dir = DATASET_PATH + "adult_dataset" + "/" + "test/"

BASEDIR_MODELS = "/exploiting_model_multiplicity/models/"


testloader = get_data(test_dir, use_batch=False, batch_size=6)

input_size = testloader.dataset.X.shape[1]
hidden_sizes = [40, 20]
output_size = 1
# TODO:
# init the model separately
# either with file
# or with function in separate python script
# script takes input parameters and returns a init model
model = nn.Sequential(
    nn.Linear(input_size, hidden_sizes[0]),
    nn.ReLU(),
    nn.Linear(hidden_sizes[0], hidden_sizes[1]),
    nn.ReLU(),
    nn.Linear(hidden_sizes[1], output_size),
    nn.Sigmoid(),
)

device = (
    "cuda" if cuda.is_available() else "mps" if backends.mps.is_available() else "cpu"
)


def get_models(project_name: str) -> list:
    # Walk through the folder and its subfolders
    models = []

    for root, dirs, files in os.walk(BASEDIR_MODELS):
        for file in files:
            if project_name in root:
                path = os.path.join(root, file)
                print(path)
                models.append(path)
    return models


def save_results(models: list, index: list, predictions: list):
    df = pd.DataFrame(predictions, columns=index, index=models)
    return df


# Print the list of file paths
# for path in file_paths:
#     print(path)
# path = "/exploiting_model_multiplicity/models/Lr-Batch/Sequential_epoch35.pt"
file_paths = get_models("Lr-Batch")
predictions = []
models_index = [path.split("/")[-1] for path in file_paths]
file_paths = ["/exploiting_model_multiplicity/models/Lr-Batch/Sequential_epoch35.pt"]
for path in file_paths:
    print(f"model loaded from path {path}")
    model.load_state_dict(load(path))
    model.eval()
    cuda.empty_cache()
    device = (
        "cuda"
        if cuda.is_available()
        else "mps"
        if backends.mps.is_available()
        else "cpu"
    )

    with no_grad():
        # for t, data in enumerate(tk):
        for data in testloader:
            inputs, labels, index = data
            print("len inputs: ", len(inputs))
            print("len labels: ", len(labels))
            ix = index.tolist()
            print("len index list: ", len(ix))
            print(ix[0][:10])
            print("type index: ", type(index))
            inputs, labels = inputs.to(device), labels.to(device)
            # tenson returned
            outputs = model(inputs)
            # print(outputs.flatten())
            # print(outputs.tolist())
            pred_proba = outputs.flatten().tolist()
            print("len pred_proba: ", len(pred_proba))
            index = index.flatten().tolist()
            print("len labels: ", len(labels))
            print("len index: ", len(index))
            predictions.append(pred_proba)
            print("len predictions: ", len(predictions))
print(models_index)
# print(index)
df = save_results(models_index, ix[0], predictions)
print(df.head())

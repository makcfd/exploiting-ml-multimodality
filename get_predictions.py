import argparse
import os

import pandas as pd
import torch
import tqdm
from torch import backends, cuda, load, nn, no_grad

from my_utilities.config_reader import read_config
from my_utilities.dataproviderNN_old import get_data

# CONST VARIABLES
DATASET_PATH = "/exploiting_model_multiplicity/data/"
test_dir = DATASET_PATH + "adult_dataset" + "/" + "test/"

BASEDIR_MODELS = "/exploiting_model_multiplicity/models/"
BASEDIR_PREDICTIONS = "/exploiting_model_multiplicity/experiment_results/"


testloader = get_data(test_dir, use_batch=False, batch_size=6)


device = (
    "cuda" if cuda.is_available() else "mps" if backends.mps.is_available() else "cpu"
)


def get_downloaded_models(project_name: str) -> list:
    # Walk through the folder and its subfolders
    models = []
    for root, dirs, files in os.walk(BASEDIR_MODELS):
        for file in files:
            if project_name in root and not file.endswith(".csv"):
                path = os.path.join(root, file)
                models.append(path)
    return models


def _save_results(models: list, index: list, predictions: list):
    df = pd.DataFrame(predictions, columns=index, index=models)
    return df


def get_predictions(args):
    config = read_config(args.config)
    project_name = config.get("project")
    list_of_models = get_downloaded_models(project_name=project_name)
    print("list_of_models: ", list_of_models)
    idx = []
    predictions = []
    # get models names to use it as index in dataframe at the end
    models_index = [path.split("/")[-1] for path in list_of_models]
    # file_paths = [
    #     "/exploiting_model_multiplicity/models/Lr-Batch/Sequential_epoch35.pt"
    # ]
    for model_path in list_of_models:
        print(f"model loaded from path {model_path}")
        model = torch.jit.load(model_path)
        model.eval()
        # model.load_state_dict(load(path))
        # model.eval()
        cuda.empty_cache()
        device = (
            "cuda"
            if cuda.is_available()
            else "mps"
            if backends.mps.is_available()
            else "cpu"
        )

        with no_grad():
            # for t, data in enumerate(tk):
            for data in testloader:
                inputs, labels, index = data
                print("len inputs: ", len(inputs))
                print("len labels: ", len(labels))
                print("len index: ", len(index))
                ix = index.tolist()
                print("len index list: ", len(ix))
                print(ix[0][:10])
                inputs, labels = inputs.to(device), labels.to(device)
                # tenson returned
                # TODO-3 May be parallel here for all models? not for one?
                outputs = model(inputs)
                print("sample outputs original: ", outputs[0:10])
                print("sample outputs flatten: ", outputs[0:10].flatten())
                print("sample outputs flatten list: ", outputs[0:10].flatten().tolist())
                pred_proba = outputs.flatten().tolist()
                print("len pred_proba: ", len(pred_proba))
                print("len labels: ", len(labels))
                predictions.append(pred_proba)
                print("len predictions: ", len(predictions))
                idx = ix[0]
    print(models_index)
    df = _save_results(models_index, idx, predictions)
    path_to_save = os.path.join(BASEDIR_PREDICTIONS)
    df.to_csv(path_to_save + f"/{project_name}.csv", index=True)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Getting predictions for the experiment"
    )
    parser.add_argument(
        "--config", help="Config/Experiment name", type=str, default=None
    )
    args = parser.parse_args()
    get_predictions(args)

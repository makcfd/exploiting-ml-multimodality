
# Implementation points  TODO:

- [x] Run simple models
- [x] Create simple NN
- [x] Define necessary fucntions and classes
- [x] Intergrate W and B to save result of training (WandB Upload)
- [ ] Create model loader. See details below
- [ ] Setup model registry on WandB
- [ ] Download models from WandB registry (WandB Download)
- [ ] Move to scripts with Argparse to run experiments on server
- [ ] Implement torch compile
- [ ] Run wandb init for each experimtn, give it a name as aexperiment 
- [ ] Run wadnb finish experiment


1. Create model loader
Use Python OrderedDict as NN model architecture to experiment with different NN models setups
Also try to use ml_collections config as blueprint for architecture

'''from collections import OrderedDict
model = nn.Sequential(OrderedDict([
                      ('fc1', nn.Linear(input_size, hidden_sizes[0])),
                      ('relu1', nn.ReLU()),
                      ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),
                      ('relu2', nn.ReLU()),
                      ('output', nn.Linear(hidden_sizes[1], output_size)),
                      ('softmax', nn.Softmax(dim=1))]))

model'''

2.         Argparse flags
        ```python
        wandb.init()
        wandb.config.epochs = 4

        parser = argparse.ArgumentParser()
        parser.add_argument(
            "-b",
            "--batch-size",
            type=int,
            default=8,
            metavar="N",
            help="input batch size for training (default: 8)",
        )
        args = parser.parse_args()
        wandb.config.update(args)
        ```


2. visualise neurons from NN to check if they "fire"
3. save trained models that performs >= to baseline and save the hyperparams with which it has been trained
4. use saved models to get predictions
5. save predictions
6. analyse predictions -> i.e evaluate prediction multiplicity 





## Useful links 

2. https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel

3. https://pytorch.org/docs/stable/notes/randomness.html

4. Thesis experiment https://pytorch.org/docs/stable/notes/randomness.html#avoiding-nondeterministic-algorithms

5. How to set random https://wandb.ai/sauravmaheshkar/RSNA-MICCAI/reports/How-to-Set-Random-Seeds-in-PyTorch-and-Tensorflow--VmlldzoxMDA2MDQy

6. https://medium.com/swlh/alpine-slim-stretch-buster-jessie-bullseye-bookworm-what-are-the-differences-in-docker-62171ed4531d

7.  https://mlinproduction.com/docker-for-ml-part-3/

8. do many changes on the model and save results of instance prediction and how it shifts 
9. do many changes on the model and save resulta on all test predicton, accumulate instability. Instability can be measured like in a paper Prediction Miltiplicity 

10. Datamodels: https://www.youtube.com/watch?v=1djD0kK2uik


11. Config usage and project example https://github.com/Gladiator07/Kaggle-Template/blob/main/src/train.py



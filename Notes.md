# Thesis points
1. Add section Limitations. Should consist of limitations 
    that has been discovered or applied during the thesis time.
2.  

# Implementation points

1. Use Python OrderedDict as NN model architecture to experiment with different NN models setups

'''from collections import OrderedDict
model = nn.Sequential(OrderedDict([
                      ('fc1', nn.Linear(input_size, hidden_sizes[0])),
                      ('relu1', nn.ReLU()),
                      ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),
                      ('relu2', nn.ReLU()),
                      ('output', nn.Linear(hidden_sizes[1], output_size)),
                      ('softmax', nn.Softmax(dim=1))]))

model'''

2. https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel

3. https://pytorch.org/docs/stable/notes/randomness.html

4. Thesis experiment https://pytorch.org/docs/stable/notes/randomness.html#avoiding-nondeterministic-algorithms

5. How to set random https://wandb.ai/sauravmaheshkar/RSNA-MICCAI/reports/How-to-Set-Random-Seeds-in-PyTorch-and-Tensorflow--VmlldzoxMDA2MDQy

6. https://medium.com/swlh/alpine-slim-stretch-buster-jessie-bullseye-bookworm-what-are-the-differences-in-docker-62171ed4531d

7.  https://mlinproduction.com/docker-for-ml-part-3/

8. do many changes on the model and save results of instance prediction and how it shifts 
9. do many changes on the model and save resulta on all test predicton, accumulate instability. Instability can be measured like in a paper Prediction Miltiplicity 

10. Datamodels: https://www.youtube.com/watch?v=1djD0kK2uik


11. Config usage and project example https://github.com/Gladiator07/Kaggle-Template/blob/main/src/train.py

12.         Argparse flags
        ```python
        wandb.init()
        wandb.config.epochs = 4

        parser = argparse.ArgumentParser()
        parser.add_argument(
            "-b",
            "--batch-size",
            type=int,
            default=8,
            metavar="N",
            help="input batch size for training (default: 8)",
        )
        args = parser.parse_args()
        wandb.config.update(args)
        ```

Implementaion TODO:
1. intergrate W and B (or alternative) ?: s
2. visualise neurons from NN to check if they "fire"
3. save trained models that performs >= to baseline and save the hyperparams with which it has been trained
4. use saved models to get predictions
5. save predictions
6. analyse predictions -> i.e evaluate prediction multiplicity 



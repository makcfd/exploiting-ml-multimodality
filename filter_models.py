import argparse
import os

import pandas as pd

from my_utilities.config_reader import read_config


"""
Script will create csv file with filtered models from the experiment
defined in yaml file and setting provided in get_multimodels() function.
"""

BASE_PATH = "/exploiting_model_multiplicity/models"
FILE_SUFFIX = ".csv"


def calculate_treshold(dataframe, metric_name, margin, reverse):
    """Out of best model calculating treshold to select models"""
    best_metric_value = sorted(dataframe[metric_name], reverse=reverse)[0]
    return best_metric_value * (1 - margin)


def find_best_experiments(
    experiments, models_metadata, resulting_df, metric, decreasing
):
    for experiment in experiments:
        best = models_metadata[models_metadata["experiment"] == experiment].sort_values(
            by=[metric], ascending=decreasing
        )[:1]
        if len(best) != 0:
            resulting_df.append(best)
    return resulting_df


def select_with_margin(
    experiments: list,
    models_metadata,
    resulting_df,
    metric: str,
    margin: int,
    decreasing: bool,
):
    if decreasing:
        treshold = calculate_treshold(models_metadata, metric, margin, reverse=False)
        df = models_metadata[models_metadata[metric] <= treshold]
        resulting_df = find_best_experiments(
            experiments, df, resulting_df, metric, decreasing
        )
    elif not decreasing:
        treshold = calculate_treshold(models_metadata, metric, margin, reverse=True)
        df = models_metadata[models_metadata[metric] >= treshold]
        resulting_df = find_best_experiments(
            experiments, df, resulting_df, metric, decreasing
        )
    return resulting_df


def get_multimodels(
    dataset_name: str,
    project_name: str,
    metric: str,
    margin: int = None,
    metric_decreasing: bool = False,
):
    path_read = os.path.join(
        BASE_PATH,
        dataset_name,
        project_name,
        "metadata",
        "all_models" + FILE_SUFFIX,
    )
    meta = pd.read_csv(path_read)
    path_write = os.path.join(
        BASE_PATH,
        dataset_name,
        project_name,
        "metadata",
        "all_models_filtered" + FILE_SUFFIX,
    )
    meta["experiment"] = [val.strip(".pt").split("-")[1] for val in meta["Unnamed: 0"]]
    experiments = meta["experiment"].unique()
    new_df = []
    if metric in meta.columns:
        if margin is not None:
            new_df = select_with_margin(
                experiments, meta, new_df, metric, margin, metric_decreasing
            )
            result = pd.concat(new_df)
            result.to_csv(path_write, index=False)
        elif margin is None:
            new_df = find_best_experiments(
                experiments, meta, new_df, metric, metric_decreasing
            )
            result = pd.concat(new_df)
            result.to_csv(path_write, index=False)
        return path_write
    print("There is no such metric")
    return -1


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Process data with input parameters")
    parser.add_argument("--config", help="Config name", type=str, default=None)
    parser.add_argument("--margin", help="Metric margin", type=float, default=0.02)
    args = parser.parse_args()
    config = read_config(args.config)
    # TODO get dataset name from config
    # form path with dedicated fucntion
    dataset_name = config.get("parameters").get("dataset").get("value")
    project_name = config.get("project")
    get_multimodels(
        dataset_name,
        project_name,
        metric="eval_acc",
        margin=args.margin,
        metric_decreasing=False,
    )

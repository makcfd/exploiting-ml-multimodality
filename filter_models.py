import argparse
import os

import pandas as pd

from my_utilities.config_reader import read_config

BASE_PATH_SAVE_FILTERED_META = "/exploiting_model_multiplicity/models/Batches/metadata"
FILE_SUFFIX = ".csv"


def calculate_treshold(dataframe, metric_name, margin, reverse):
    best_metric_value = sorted(dataframe[metric_name], reverse=reverse)[0]
    return best_metric_value * (1 - margin)


def find_best_experiments(experiments, filtered_df, resulting_df, metric, decreasing):
    for experiment in experiments:
        best = filtered_df[filtered_df["experiment"] == experiment].sort_values(
            by=[metric], ascending=decreasing
        )[:1]
        if len(best) != 0:
            resulting_df.append(best)
    return resulting_df


def get_multimodels(
    project: str,
    metric: str,
    margin: int,
    metric_decreasing: bool,
) -> dict:
    """
    Returns a dictionary that contains models which
    eval metric is in range of margin comapring to the top model
    Inputs:
        - path_candidates: path to csv file with models metadata
        - margin: defined deviation of eval metric from top one
        - metric_decreasing: if eval metric better if low of high
    Outputs:
        - multimodel: dict that containes only models that have eval
                      metrics within the defined margin
    """

    data_path = os.path.join(BASE_PATH_SAVE_FILTERED_META, project + FILE_SUFFIX)
    meta = pd.read_csv(data_path)
    path_results = os.path.join(
        BASE_PATH_SAVE_FILTERED_META, project + "_filtered" + FILE_SUFFIX
    )
    multimodel = {}
    meta["experiment"] = [val.strip(".pt").split("-")[1] for val in meta["Unnamed: 0"]]
    experiments = meta["experiment"].unique()
    new_df = []
    if metric in meta.columns:
        if metric_decreasing:
            treshold = calculate_treshold(meta, metric, margin, reverse=False)
            df = meta[meta[metric] <= treshold]
            new_df = find_best_experiments(
                experiments, df, new_df, metric, metric_decreasing
            )
        elif not metric_decreasing:
            treshold = calculate_treshold(meta, metric, margin, reverse=True)
            df = meta[meta[metric] >= treshold]
            new_df = find_best_experiments(
                experiments, df, new_df, metric, metric_decreasing
            )
        if len(new_df) == len(experiments):
            result = pd.concat(new_df)
            result.to_csv(path_results, index=False)
            return path_results
        else:
            new_df = []
            new_df = find_best_experiments(
                experiments, meta, new_df, metric, metric_decreasing
            )
            result = pd.concat(new_df)
            result.to_csv(path_results, index=False)
            return path_results
    print("There is no such metric")
    return -1


# project = "Batches"

# get_multimodels(
#     project,
#     metric="eval_acc",
#     margin=0.03,
#     metric_decreasing=False,
# )


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Process data with input parameters")
    parser.add_argument("--config", help="Config name", type=str, default=None)
    args = parser.parse_args()
    config = read_config(args.config)
    get_multimodels(
        config["project"],
        metric="eval_acc",
        margin=0.03,
        metric_decreasing=False,
    )

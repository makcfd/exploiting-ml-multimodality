import argparse
import os

import pandas as pd

from my_utilities.config_reader import read_config


"""
Script will create csv file with filtered models from the experiment
defined in yaml file and setting provided in get_multimodels() function.
"""

BASE_PATH_SAVE_FILTERED_META = "/exploiting_model_multiplicity/models"
FILE_SUFFIX = ".csv"


def calculate_treshold(dataframe, metric_name, margin, reverse):
    """Out of best model calculating treshold to select models"""
    best_metric_value = sorted(dataframe[metric_name], reverse=reverse)[0]
    return best_metric_value * (1 - margin)


def find_best_experiments(
    experiments, models_metadata, resulting_df, metric, decreasing
):
    for experiment in experiments:
        best = models_metadata[models_metadata["experiment"] == experiment].sort_values(
            by=[metric], ascending=decreasing
        )[:1]
        if len(best) != 0:
            resulting_df.append(best)
    return resulting_df


def select_with_margin(
    experiments: list,
    models_metadata,
    resulting_df,
    metric: str,
    margin: int,
    decreasing: bool,
):
    if decreasing:
        treshold = calculate_treshold(models_metadata, metric, margin, reverse=False)
        df = models_metadata[models_metadata[metric] <= treshold]
        resulting_df = find_best_experiments(
            experiments, df, resulting_df, metric, decreasing
        )
    elif not decreasing:
        treshold = calculate_treshold(models_metadata, metric, margin, reverse=True)
        df = models_metadata[models_metadata[metric] >= treshold]
        resulting_df = find_best_experiments(
            experiments, df, resulting_df, metric, decreasing
        )
    return resulting_df


def get_multimodels(
    project: str,
    metric: str,
    margin: int = None,
    metric_decreasing: bool = False,
) -> dict:
    """
    Returns a dictionary that contains models which
    eval metric is in range of margin comapring to the top model
    Inputs:
        - path_candidates: path to csv file with models metadata
        - margin: defined deviation of eval metric from top one
        - metric_decreasing: if eval metric better if low of high
    Outputs:
        - multimodel: dict that containes only models that have eval
                      metrics within the defined margin
    """

    data_path = os.path.join(
        BASE_PATH_SAVE_FILTERED_META, project, "metadata", project + FILE_SUFFIX
    )
    meta = pd.read_csv(data_path)
    path_results = os.path.join(
        BASE_PATH_SAVE_FILTERED_META,
        project,
        "metadata",
        project + "_filtered" + FILE_SUFFIX,
    )
    # multimodel = {}
    meta["experiment"] = [val.strip(".pt").split("-")[1] for val in meta["Unnamed: 0"]]
    experiments = meta["experiment"].unique()
    new_df = []
    if metric in meta.columns:
        if margin is not None:
            new_df = select_with_margin(
                experiments, meta, new_df, metric, margin, metric_decreasing
            )
            result = pd.concat(new_df)
            result.to_csv(path_results, index=False)
        elif margin is None:
            new_df = find_best_experiments(
                experiments, meta, new_df, metric, metric_decreasing
            )
            result = pd.concat(new_df)
            result.to_csv(path_results, index=False)
        return path_results
    print("There is no such metric")
    return -1


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Process data with input parameters")
    parser.add_argument("--config", help="Config name", type=str, default=None)
    parser.add_argument("--margin", help="Metric margin", type=float, default=0.02)
    args = parser.parse_args()
    config = read_config(args.config)
    get_multimodels(
        config["project"],
        metric="eval_acc",
        margin=args.margin,
        metric_decreasing=False,
    )

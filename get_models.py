import argparse
import os

import pandas as pd

import wandb
from my_utilities.config_reader import read_config


BASE_PATH_TO_SAVE = "/exploiting_model_multiplicity/models"


def _save_models_metadata(models_metadata: dict, project_name: str):
    meta = [v for v in models_metadata.values()]
    df = pd.DataFrame(meta, index=models_metadata.keys())

    path_to_save = os.path.join(BASE_PATH_TO_SAVE, project_name, "metadata")
    if not os.path.exists(path_to_save):
        os.makedirs(path_to_save)
    df.to_csv(path_to_save + f"/{project_name}.csv", index=True)


def _get_candidates(
    api,
    project_name: str,
    metric: str = None,
    metric_decreasing: bool = True,
    top_n_artefacts: int = 1,
    filter_runs_name: list = [],
) -> dict:
    """
    Returns dictionary consists of multimodel candidates from each experiment
    within one project
    Inputs:
        - api : instance of WandB API Class to connect to WandB Cloud
        - project_name: name of wandb project
        - metric: ML metric name that will be used to retrive
                  the most suatable candidates
        - metric_decreasing: select model whether metric should be lower of higher
                             defaul = True
        - top_n_artefacts: define how many candidates to retrive from one experiment
                            within the project
                             default = 1
        - filter_runs_name: define list of experiments what must be used
                            default = []
    Outputs:
        - all_models: dict that containes only models that satisfies
                      passed parameters.
                      Keys of the dict are the models (artifacts) names
                      Values of the dict are the mertic values
    """
    metric_decreasing = (
        True  # decreasing: If decreasing is `True`, then lower metric is better
    )
    all_models = {}
    runs = api.runs(project_name)
    models_metadata = {}
    if filter_runs_name:
        # filter runs
        runs = [run for run in runs if run.name in filter_runs_name]
    for run in runs:
        print("run selected:", run.name)
        models_from_run = {}

        for artifact in run.logged_artifacts():
            # from artefact can get https://docs.wandb.ai/ref/python/artifact#docusaurus_skipToContent_fallback
            if artifact.type == "model":
                # selecting top models within one run
                models_metadata[artifact.name] = artifact.metadata
                if len(models_from_run) < top_n_artefacts:
                    models_from_run[artifact.name] = artifact.metadata[metric]
                else:
                    if metric_decreasing:
                        max_value_key = max(models_from_run)
                        if artifact.metadata[metric] < models_from_run[max_value_key]:
                            del models_from_run[max_value_key]
                            models_from_run[artifact.name] = artifact.metadata[metric]
                    if not metric_decreasing:
                        min_value_key = min(models_from_run)
                        if models_from_run[min_value_key] < artifact.metadata[metric]:
                            del models_from_run[min_value_key]
                            models_from_run[artifact.name] = artifact.metadata[metric]
        all_models.update(models_from_run)
    _save_models_metadata(models_metadata, project_name)
    return all_models


def _get_multimodels(candidates: dict, margin: int, metric_decreasing: bool) -> dict:
    """
    Returns a dictionary that contains models which
    eval metric is in range of margin comapring to the top model
    Inputs:
        - candidates: dict of all candidates where k - name and v - eval metric
        - margin: defined deviation of eval metric from top one
        - metric_decreasing: if eval metric better if low of high
    Outputs:
        - multimodel: dict that containes only models that have eval
                      metrics within the definedmargin
    """
    multimodel = {}
    if metric_decreasing:
        best_value_key = min(candidates)
        multimodel[best_value_key] = candidates[best_value_key]
    else:
        best_value_key = max(candidates)
        multimodel.update(candidates[best_value_key])
    best_value = candidates[best_value_key]
    for k, v in candidates.items():
        if metric_decreasing and v < best_value * (1 + margin):
            multimodel[k] = candidates[k]
        elif not metric_decreasing and best_value * (1 - margin) < v:
            multimodel[k] = candidates[k]
    return multimodel


def _download_models(api, project: str, multimodel: dict) -> str:
    """
    Downloads models into the defined directory.
    Inputs:
        - multimodel: dictionary where keys are names of artifacts
        - path_to_save: directory where models will be saved
    Outputs:
        - path where downloaded models are saved

    """

    path_to_save = os.path.join(BASE_PATH_TO_SAVE, project)
    if not os.path.exists(path_to_save):
        os.makedirs(path_to_save)
    count = 0
    for model_name in multimodel.keys():
        try:
            artifact = api.artifact(f"makcfd/{project}/{model_name}")
            artifact.download(root=path_to_save)
            count += 1
        except ValueError:
            print("There is no artifact with such a name :(")

        finally:
            print(f"Downloaded artifacts: {count} , name: {model_name}")
    return path_to_save


def get_models():
    key = "620527a80f5b194ce6ba9498879a2ebe65db428d"
    api = wandb.Api(api_key=key)

    parser = argparse.ArgumentParser(
        description="Getting trained modes from the experiment"
    )
    parser.add_argument(
        "--config",
        help="Config/Experiment name",
        type=str,
        default=None,
    )
    args = parser.parse_args()
    config = read_config(args.config)

    project_name = config.get("project")
    all_candidates = _get_candidates(
        api,
        project_name=project_name,
        metric="eval_acc",
        metric_decreasing=True,
        top_n_artefacts=999,
        filter_runs_name=[],
    )
    # multimodel_margin = _get_multimodels(
    #     all_candidates,
    #     100,
    #     True,
    # )

    _download_models(
        api,
        project=project_name,
        multimodel=all_candidates,
    )


get_models()

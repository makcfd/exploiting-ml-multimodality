import os
import pandas as pd
import numpy as np


def save_experimet_results(
    features: pd.DataFrame,
    target: np.array,
    predicted: np.array,
    *args,
) -> None:
    """
    Solely saves prediction results of validation/test dataset in csv file.
    File named with model hyperparameters.
    Support only binary classification.

    Parameters:
    -----------
    features - validation/test set of features in DataFrame format
    target - np.array of target classes
    predicted - np.array of predicted classes
    args - set of hyperparams used to train a model

    Output:
    -----------
    csv file saved in directory: /exploiting_model_multiplicity/experiment_results/
    """
    results_df = features.copy(deep=True)
    results_df["target"] = target
    results_df["predicted"] = predicted
    # features_valid["prob_0"] = probabilities_valid[:,0]
    # features_valid["prob_1"] = probabilities_valid[:,1]
    results = results_df[
        [
            "target",
            "predicted",
        ]
    ]
    file_name = str()
    for arg in args:
        file_name += str(arg) + "_"
    results.to_csv(
        f"/exploiting_model_multiplicity/experiment_results/{file_name}.csv", index=True
    )
    del results_df


def find_difference(dir_path: str) -> None:
    """Find differences in predictions

    Parameters
    ----------
    dir_path : str, required
        Path to a directory where experiment predictions are stored
        Expected to get csv files for an analisys
        CSV files must be of equal length

    Returns
    ------
    two objects:
        dictionary contains found differences with original indexes
        list of files that has been processed
    """
    # dir_path = "/exploiting_model_multiplicity/experiment_results"

    # Get a list of all csv files in the directory
    csv_files = [f for f in os.listdir(dir_path) if f.endswith(".csv")]

    # Read the first csv file to start the comparison
    df = pd.read_csv(os.path.join(dir_path, csv_files[0]), index_col=0)
    print("file results to compare: ", csv_files[0])
    print("Number of test observations: ", df.shape[0])
    # Column to compare
    # get column by the name !!!!!!!!!!!!!!!!!!!!!!!
    # col_to_compare = df.columns[1]

    # print("Column to compare: ", col_to_compare)
    # Store file names and indices where difference has been found
    diff_indices = {}

    # Iterate over remaining csv files
    for file in csv_files[1:]:
        temp_df = pd.read_csv(os.path.join(dir_path, file), index_col=0)
        # print("Comparing with: ", file)

        # If a difference is found, store the file name and the differing indices
        if not df["predicted"].equals(temp_df["predicted"]):
            diff_indices[file] = (
                df["predicted"].compare(temp_df["predicted"]).index.to_list()
            )
            print(f"Comparing with: {file} found {len(diff_indices[file])} differences")

    # Print the files and indices where differences were found
    for file, indices in diff_indices.items():
        print(f"{len(indices)} differences found in file: {file} at indices {indices}")
    return diff_indices, csv_files

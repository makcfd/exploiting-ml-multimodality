import pandas as pd
import numpy as np
from torch.utils.data import Dataset
from torch.utils.data import DataLoader, RandomSampler
import torch


# use case 1
"""Normal dataloader with batch size"""
# DATASET_PATH = "/exploiting_model_multiplicity/data/"
# test_dir = DATASET_PATH + "adult_dataset" + "/" + "test/"
# testloader = get_data(
#     test_dir,
#     batch_size=3,
#     random=False,
# )

# for i, data in enumerate(testloader):
#     f, l, id = data
#     print(i, l.flatten(), id.flatten())


# Use case 2
"""
Random dataloader
-----------------
Behavior specification (no documentation given)
RandomSampler slices the dataset size to the num_samples it was given
Dataloader will not output more samples than RandomSampler can give
If RandomSampler.num_samples = 3 and Dataloader.batch_size = 3
Dataloader will have one iteration and output all 3 sample
If RandomSampler.num_samples = 3 and Dataloader.batch_size = 1
Dataloader will have 3 iterations and output 1 sample per iteration
"""
# DATASET_PATH = "/exploiting_model_multiplicity/data/"
# test_dir = DATASET_PATH + "adult_dataset" + "/" + "test/"
# testloader = get_data(
#     test_dir,
#     batch_size=10,
#     random=True,
#     size=10,
# )
# for i, data in enumerate(testloader):
#     f, l, id = data
#     print(i, l.flatten(), id.flatten())


class Data(Dataset):
    def __init__(self, X, y):
        # need to convert float64 to float32 else
        # will get the following error
        # RuntimeError: expected scalar type Double but found Float
        self.index = X.index.values
        X = X.values
        y = y.values
        self.X = torch.from_numpy(X.astype(np.float32))
        # need to convert float64 to Long else
        # will get the following error
        # RuntimeError: expected scalar type Long but found Float
        self.y = torch.from_numpy(y).type(torch.LongTensor)
        self.len = self.X.shape[0]

    def __getitem__(self, index):
        return self.X[index], self.y[index], self.index[index]

    def __len__(self):
        return self.len


def create_loader(**kargs):
    return DataLoader(**kargs)


def get_data(
    directory: str,
    batch_size: int = 1,
    random: bool = False,
    size: int = 1,
    seed_value: int = 42,
) -> DataLoader:
    subset_sampler = None
    X = pd.read_csv(directory + "X.csv", index_col=0)
    y = pd.read_csv(directory + "Y.csv", index_col=0)
    data = Data(X, y)
    if random:
        torch.manual_seed(seed_value)
        random_sampler = RandomSampler(data, replacement=False, num_samples=size)

    else:
        random_sampler = None

    loader = create_loader(
        dataset=data, batch_size=batch_size, shuffle=False, sampler=random_sampler
    )
    return loader


# Use case 2
"""
Random dataloader
-----------------
Behavior specification (no documentation given)
RandomSampler slices the dataset size to the num_samples it was given
Dataloader will not output more samples than RandomSampler can give
If RandomSampler.num_samples = 3 and Dataloader.batch_size = 3
Dataloader will have one iteration and output all 3 sample
If RandomSampler.num_samples = 3 and Dataloader.batch_size = 1
Dataloader will have 3 iterations and output 1 sample per iteration
"""
# DATASET_PATH = "/exploiting_model_multiplicity/data/"
# test_dir = DATASET_PATH + "adult_dataset" + "/" + "test/"
# testloader = get_data(
#     test_dir,
#     batch_size=1,
#     random=True,
#     size=10,
# )
# for i, data in enumerate(testloader):
#     f, l, id = data
#     print(i, l.flatten(), id.flatten())

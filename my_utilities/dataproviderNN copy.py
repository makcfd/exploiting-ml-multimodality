import pandas as pd
import numpy as np
from torch.utils.data import Dataset
from torch.utils.data import DataLoader, SubsetRandomSampler, RandomSampler
import torch
import random

DATASET_PATH = "/exploiting_model_multiplicity/data/"
test_dir = DATASET_PATH + "adult_dataset" + "/" + "test/"
df_X = pd.read_csv(test_dir + "X.csv", index_col=0)

indices = df_X.index.to_list()

# Shuffle the indices randomly.
random.shuffle(indices)

# Select the first 3000 shuffled indices to create a subset.
subset_indices = indices[:5]
print(subset_indices)
# Create a DataLoader for the subset using SubsetRandomSampler.
subset_sampler = SubsetRandomSampler(subset_indices)
subset_loader = DataLoader(df_X, batch_size=1, sampler=subset_sampler, shuffle=False)

# # You can now iterate through 'subset_loader' to access the randomly selected 3000 observations.
# for batch in subset_loader:
#     print(batch)


class Data(Dataset):
    def __init__(self, X, y):
        # need to convert float64 to float32 else
        # will get the following error
        # RuntimeError: expected scalar type Double but found Float
        self.index = X.index.values
        X = X.values
        y = y.values
        self.X = torch.from_numpy(X.astype(np.float32))
        # need to convert float64 to Long else
        # will get the following error
        # RuntimeError: expected scalar type Long but found Float
        self.y = torch.from_numpy(y).type(torch.LongTensor)
        self.len = self.X.shape[0]

    def __getitem__(self, index):
        return self.X[index], self.y[index], self.index

    def __len__(self):
        return self.len


def get_data(
    directory: str,
    use_batch: bool = True,
    batch_size: int = 1,
    random: bool = False,
    size: int = 1,
) -> DataLoader:
    print("dataloader received info - use_batch: ", use_batch)
    print("dataloader received info - batch_size: ", batch_size)
    if random:
        df_X = pd.read_csv(directory + "X.csv", index_col=0)
        df_y = pd.read_csv(directory + "Y.csv", index_col=0)
        indices = list(range(len(df_X)))

        # Shuffle the indices randomly.
        # random.shuffle(indices)

        # Select the first 3000 shuffled indices to create a subset.
        subset_indices = indices[:size]
        print(subset_indices)
        # Create a DataLoader for the subset using SubsetRandomSampler.
        subset_sampler = SubsetRandomSampler(subset_indices)
        # subset_loader = DataLoader(df_X, batch_size=1, sampler=subset_sampler, shuffle=False)

    X = pd.read_csv(directory + "X.csv", index_col=0)
    y = pd.read_csv(directory + "Y.csv", index_col=0)
    data = Data(X, y)

    loader = DataLoader(
        data,
        batch_size=1,
        shuffle=False,
        sampler=subset_sampler,
    )
    return loader


# # # Below is test fixtures for this file
DATASET_PATH = "/exploiting_model_multiplicity/data/"
test_dir = DATASET_PATH + "adult_dataset" + "/" + "test/"
# # # train_dir = DATASET_PATH + "adult_dataset" + "/" + "train/"
testloader = get_data(test_dir, use_batch=False, random=True, size=1)

# for data in testloader:
#     features, labels, index = data
# print("len index: ", len(index))
# print("values of index: ", index)
for data in testloader:
    features, labels, index = data
    print("lex index in for loop: ", len(index))
    print(index)
#     print("values of index in for loop: ", index[:1])

# df = pd.read_csv(test_dir + "X.csv", index_col=0)
# chosen_idx = np.random.choice(df.index.to_list(), replace=False, size=50)
# # print("chosen_idx:", chosen_idx)
# df2 = df.loc[chosen_idx]
# print(df2.index)

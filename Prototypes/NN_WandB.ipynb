{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "import wandb\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "#from tqdm.notebook import tqdm\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchmetrics import Accuracy, F1Score\n",
    "#from logger import CheckpointSaver\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "from ml_collections import config_dict\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "# custom project utilities\n",
    "from my_utilities.dataproviderNN import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define as iterable values\n",
    "cfg = config_dict.ConfigDict()\n",
    "cfg.epochs = {'values': [1]}\n",
    "cfg.batch_size = {'values': [8, 16, 32, 64, 128, 256]}\n",
    "cfg.lr = {'values': [1e-3, 1e-2, 1e-4, 3e-5, 3e-4]}\n",
    "# Strings must not have blank spaces. Experiment use dashes in experiment name\n",
    "cfg.dataset = {'values': ['adult_dataset']}\n",
    "cfg.seed = {'values': [42]}\n",
    " \n",
    "#cfg.optimizer = \"SGD\"\n",
    "#cfg.dropout = random.uniform(0.01, 0.80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run = wandb.init(\n",
    "#     project=\"thesis-test\", \n",
    "#     config=cfg.to_dict(),\n",
    "#     name=cfg.experiment_name,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_configuration = {\n",
    "    # q?: how this project variable impact on the logs \n",
    "    'project': 'Lr-Batch_debug',\n",
    "    'method': 'random',\n",
    "    'name': 'thesis',\n",
    "    'metric': {\n",
    "        'goal': 'minimize', \n",
    "        'name': 'eval_loss'\n",
    "        },\n",
    "}\n",
    "sweep_configuration['parameters'] = cfg.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters_dict = {\n",
    "#     'optimizer': {\n",
    "#         'values': ['adam', 'sgd']\n",
    "#         },\n",
    "#     'fc_layer_size': {\n",
    "#         'values': [128, 256, 512]\n",
    "#         },\n",
    "#     'dropout': {\n",
    "#           'values': [0.3, 0.4, 0.5]\n",
    "#         },\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"620527a80f5b194ce6ba9498879a2ebe65db428d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: ewu0tos5\n",
      "Sweep URL: https://wandb.ai/makcfd/Lr-Batch_debug/sweeps/ewu0tos5\n",
      "sweep_id:  ewu0tos5\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_configuration,\n",
    "                       project=sweep_configuration['project'],\n",
    "                       )\n",
    "print(\"sweep_id: \", sweep_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define a field in the config\n",
    "# # later use it to define architecture of the model\n",
    "# input_size = 10\n",
    "# hidden_sizes = [10,20]\n",
    "# output_size = 1\n",
    "# cfg_field = config_dict.FieldReference(OrderedDict())\n",
    "# cfg = config_dict.ConfigDict({\n",
    "#     'model': cfg_field,\n",
    "# })\n",
    "# cfg.model = OrderedDict([('fc1', nn.Linear(input_size, hidden_sizes[0])),\n",
    "#              ('relu1', nn.ReLU()),\n",
    "#              ('relu2', nn.ReLU()),\n",
    "#              ('output', nn.Linear(hidden_sizes[1], output_size)),\n",
    "#              ('softmax', nn.Softmax(dim=1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 10\n",
    "output_size = 1\n",
    "hidden_sizes = [40,20]\n",
    "model_schm_1 = OrderedDict(\n",
    "    [\n",
    "        ('fc1', nn.Linear(input_size, hidden_sizes[0])),\n",
    "        ('relu1', nn.ReLU()),\n",
    "        ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "        ('relu2', nn.ReLU()),\n",
    "        ('output', nn.Linear(hidden_sizes[1], output_size)),\n",
    "        ('softmax', nn.Softmax(dim=1))\n",
    "    ]\n",
    ")\n",
    "model_schm_2 = OrderedDict(\n",
    "    [\n",
    "        ('fc1', nn.Linear(input_size, hidden_sizes[0])),\n",
    "        ('relu1', nn.ReLU()),\n",
    "        ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "        ('relu2', nn.ReLU()),\n",
    "        ('output', nn.Linear(hidden_sizes[1], output_size)),\n",
    "        ('sigmoid', nn.Sigmoid())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 42\n"
     ]
    }
   ],
   "source": [
    "# Dataloader randomness \n",
    "# if num_workers >1 https://pytorch.org/docs/stable/notes/randomness.html#dataloader\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"Random seed set as {seed}\")\n",
    "set_seed(cfg.seed.to_dict()[\"values\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, criterion, train_data_loader, optimizer, epoch, device='cuda'):\n",
    "    model.train()\n",
    "    accuracy = Accuracy(task='binary')\n",
    "    f1 = F1Score(task='binary')\n",
    "    fin_loss = 0.0\n",
    "    fin_accuracy = 0.0\n",
    "    fin_f1 = 0.0\n",
    "    tk = tqdm(train_data_loader, desc=\"Epoch\" + \" [TRAIN] \" + str(epoch + 1))\n",
    "    for t, data in enumerate(tk):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        labels = labels.float()\n",
    "        #labels = labels.unsqueeze(1)\n",
    "        train_loss = criterion(outputs, labels)\n",
    "        fin_accuracy += accuracy(outputs, labels)\n",
    "        fin_f1 += f1(outputs, labels)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        fin_loss += train_loss.item()\n",
    "        #wandb.log({\"train batch loss\": train_loss.item()})\n",
    "        tk.set_postfix(\n",
    "            {\n",
    "                \"loss\": \"%.6f\" % float(fin_loss / (t + 1)),\n",
    "                \"LR\": optimizer.param_groups[0][\"lr\"],\n",
    "            }\n",
    "        )\n",
    "    return (fin_loss / len(train_data_loader),\n",
    "            fin_accuracy / len(train_data_loader),\n",
    "            fin_f1 / len(train_data_loader),\n",
    "            optimizer.param_groups[0][\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fn(model, criterion, eval_data_loader, epoch, device='cuda'):\n",
    "    model.eval()\n",
    "    accuracy = Accuracy(task='binary')\n",
    "    f1 = F1Score(task='binary')\n",
    "    fin_loss = 0.0\n",
    "    fin_accuracy = 0.0\n",
    "    fin_f1 = 0.0\n",
    "    tk = tqdm(eval_data_loader, desc=\"Epoch\" + \" [VALID] \" + str(epoch + 1))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for t, data in enumerate(tk):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            labels = labels.float()\n",
    "            # labels = labels.unsqueeze(1)\n",
    "            eval_loss = criterion(outputs, labels)\n",
    "            fin_loss += eval_loss.item()\n",
    "            fin_accuracy += accuracy(outputs, labels)\n",
    "            fin_f1 += f1(outputs, labels)\n",
    "            #wandb.log({\"eval batch loss\": eval_loss.item()})\n",
    "            tk.set_postfix({\"loss\": \"%.6f\" % float(fin_loss / (t + 1))})\n",
    "        return (fin_loss / len(eval_data_loader),\n",
    "                fin_accuracy / len(eval_data_loader),\n",
    "                fin_f1/len(eval_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckpointSaver:\n",
    "    def __init__(self, experiment, dirpath, decreasing=True, top_n=5):\n",
    "        \"\"\"\n",
    "        dirpath: Directory path where to store all model weights \n",
    "        decreasing: If decreasing is `True`, then lower metric is better\n",
    "        top_n: Total number of models to track based on validation metric value\n",
    "        \"\"\"\n",
    "        if not os.path.exists(dirpath): os.makedirs(dirpath)\n",
    "        self.dirpath = dirpath\n",
    "        self.top_n = top_n \n",
    "        self.decreasing = decreasing\n",
    "        self.top_model_paths = []\n",
    "        self.best_metric_val = np.Inf if decreasing else -np.Inf\n",
    "        self.experiment = experiment\n",
    "        \n",
    "    # add to this function metadata parameter and pass it to artefact\n",
    "    def __call__(self, model, epoch, metric_val, **kwargs):\n",
    "        #model_path = os.path.join(self.dirpath, model.__class__.__name__ + f'_epoch-{epoch}.pt')\n",
    "        model_path = os.path.join(self.dirpath, f'model-{self.experiment}-epoch-{epoch}.pt')\n",
    "        save = metric_val < self.best_metric_val if self.decreasing else metric_val > self.best_metric_val\n",
    "        if save: \n",
    "            logging.info(f\"Current metric value better than {metric_val} better than best {self.best_metric_val}, saving model at {model_path}, & logging model weights to W&B.\")\n",
    "            self.best_metric_val = metric_val\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            # TODO: save file in W&B in run\n",
    "            model_name = f'model-{self.experiment}-epoch-{epoch}.pt'\n",
    "\n",
    "            self.log_artifact_custom(model_name, model_path, metric_val, epoch, **kwargs)\n",
    "            self.top_model_paths.append({'path': model_path, 'score': metric_val})\n",
    "            self.top_model_paths = sorted(self.top_model_paths, key=lambda o: o['score'], reverse=not self.decreasing)\n",
    "        if len(self.top_model_paths)>self.top_n: \n",
    "            self.cleanup()\n",
    "    # epoch\n",
    "    # metrics \n",
    "    def log_artifact_custom(self, filename, model_path, metric_val, epoch, **kwargs):\n",
    "        # ? validation score doubled \n",
    "        metadata = {'Validation score': metric_val,\n",
    "                    \"epoch\": epoch\n",
    "                    }\n",
    "        for k, v in kwargs.items():\n",
    "            metadata[k] = v\n",
    "        artifact = wandb.Artifact(filename, type='model', metadata=metadata)\n",
    "        \n",
    "        # TODO: save on the fly\n",
    "        artifact.add_file(model_path)\n",
    "        wandb.run.log_artifact(artifact)        \n",
    "    \n",
    "    def cleanup(self):\n",
    "        to_remove = self.top_model_paths[self.top_n:]\n",
    "        logging.info(f\"Removing extra models.. {to_remove}\")\n",
    "        for o in to_remove:\n",
    "            os.remove(o['path'])\n",
    "        self.top_model_paths = self.top_model_paths[:self.top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(config=None):\n",
    "    \"\"\"\n",
    "    Function is taking care of:\n",
    "        1. Initial data reading\n",
    "        2. Data loaders preparation\n",
    "        3. Formation of model structure\n",
    "        4. Checkpoint saver init\n",
    "        5. Iteration over epochs\n",
    "        6. Calling training and evaluation\n",
    "    \"\"\"\n",
    "    DATASET_PATH = \"/exploiting_model_multiplicity/data/\"\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config) as run:\n",
    "        config = wandb.config\n",
    "        config.experiment_name = \"Experiment\" + \"-\" + run.name.split(\"-\")[2]\n",
    "        run.name = \"Experiment\" + \"-\" + run.name.split(\"-\")[2]\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        train_dir = DATASET_PATH + config.dataset + \"/\" + \"train/\"\n",
    "        test_dir = DATASET_PATH + config.dataset + \"/\" + \"test/\"\n",
    "        trainloader = get_data(train_dir, config.batch_size)\n",
    "        testloader = get_data(test_dir, config.batch_size)\n",
    "    # model\n",
    "        input_size = testloader.dataset.X.shape[1]\n",
    "        hidden_sizes = [40, 20]\n",
    "        output_size = 1\n",
    "    # TODO: pass dictionary style the model architecture to this function\n",
    "        model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(hidden_sizes[1], output_size),\n",
    "                          nn.Sigmoid())\n",
    "\n",
    "        model = model.to(device)\n",
    "        criterion = nn.BCELoss()\n",
    "    # optimizer\n",
    "    # TODO: pass optimiser to this function\n",
    "    # TODO: test if optimiser can be passed from config\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "    # checkpoint saver\n",
    "        checkpoint_saver = CheckpointSaver(config.experiment_name,\n",
    "                                           dirpath='./model_weights',\n",
    "                                           decreasing=True,\n",
    "                                           top_n=5)\n",
    "        for epoch in range(config.epochs):\n",
    "            print(f\"epoch started: {epoch}\")\n",
    "            avg_loss_train, avg_accuracy_train, avg_f1_train, lr = train_fn(\n",
    "                model, criterion, trainloader, optimizer, epoch, device=device\n",
    "            )\n",
    "            avg_loss_eval, avg_accuracy_eval, avg_f1_eval = eval_fn(model, criterion, testloader, epoch, device=device)\n",
    "            checkpoint_saver(model,\n",
    "                             epoch,\n",
    "                             metric_val = avg_loss_eval,\n",
    "                             train_loss = avg_loss_train,\n",
    "                             # eval_loss = avg_loss_eval,\n",
    "                             train_acc = avg_accuracy_train,\n",
    "                             train_f1 = avg_f1_train,\n",
    "                             eval_acc = avg_accuracy_eval,\n",
    "                             eval_f1 = avg_f1_eval)\n",
    "            \n",
    "            wandb.run.log({'epoch': epoch,\n",
    "                       'train loss': avg_loss_train,\n",
    "                       'eval loss': avg_loss_eval,\n",
    "                       'train acc': avg_accuracy_train,\n",
    "                       'train f1': avg_f1_train,\n",
    "                       'eval acc': avg_accuracy_eval,\n",
    "                       'eval f1': avg_f1_eval})\n",
    "            print(\n",
    "                f\"EPOCH = {epoch} | TRAIN_LOSS = {avg_loss_train:.2f} | EVAL_LOSS = {avg_loss_eval:.2f}\"\n",
    "            )\n",
    "            print(f\"TRAIN_ACC = {avg_accuracy_train:.2f} | EVAL_ACC = {avg_accuracy_eval:.2f}\")\n",
    "            print(f\"TRAIN_F1 = {avg_f1_train:.2f} | EVAL_F1 = {avg_f1_eval:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: t2f6d0m3 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataset: adult_dataset\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/exploiting_model_multiplicity/Prototypes/wandb/run-20230806_183028-t2f6d0m3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/makcfd/Lr-Batch_debug/runs/t2f6d0m3' target=\"_blank\">true-sweep-1</a></strong> to <a href='https://wandb.ai/makcfd/Lr-Batch_debug' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/makcfd/Lr-Batch_debug/sweeps/ewu0tos5' target=\"_blank\">https://wandb.ai/makcfd/Lr-Batch_debug/sweeps/ewu0tos5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/makcfd/Lr-Batch_debug' target=\"_blank\">https://wandb.ai/makcfd/Lr-Batch_debug</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/makcfd/Lr-Batch_debug/sweeps/ewu0tos5' target=\"_blank\">https://wandb.ai/makcfd/Lr-Batch_debug/sweeps/ewu0tos5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/makcfd/Lr-Batch_debug/runs/t2f6d0m3' target=\"_blank\">https://wandb.ai/makcfd/Lr-Batch_debug/runs/t2f6d0m3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataloader received info - directory:  /exploiting_model_multiplicity/data/adult_dataset/train/\n",
      "dataloader received info - batch_size:  16\n",
      "dataloader received info - directory:  /exploiting_model_multiplicity/data/adult_dataset/test/\n",
      "dataloader received info - batch_size:  16\n",
      "epoch started: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [TRAIN] 1: 100%|██████████| 2440/2440 [00:06<00:00, 366.84it/s, loss=0.497868, LR=0.0001]\n",
      "Epoch [VALID] 1: 100%|██████████| 610/610 [00:01<00:00, 479.86it/s, loss=0.408254]\n",
      "INFO:root:Current metric value better than 0.40825437173003054 better than best inf, saving model at ./model_weights/model-Experiment-1-epoch-0.pt, & logging model weights to W&B.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH = 0 | TRAIN_LOSS = 0.50 | EVAL_LOSS = 0.41\n",
      "TRAIN_ACC = 0.79 | EVAL_ACC = 0.80\n",
      "TRAIN_F1 = 0.31 | EVAL_F1 = 0.27\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁</td></tr><tr><td>eval acc</td><td>▁</td></tr><tr><td>eval f1</td><td>▁</td></tr><tr><td>eval loss</td><td>▁</td></tr><tr><td>train acc</td><td>▁</td></tr><tr><td>train f1</td><td>▁</td></tr><tr><td>train loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>eval acc</td><td>0.79791</td></tr><tr><td>eval f1</td><td>0.26969</td></tr><tr><td>eval loss</td><td>0.40825</td></tr><tr><td>train acc</td><td>0.78924</td></tr><tr><td>train f1</td><td>0.30756</td></tr><tr><td>train loss</td><td>0.49787</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">true-sweep-1</strong> at: <a href='https://wandb.ai/makcfd/Lr-Batch_debug/runs/t2f6d0m3' target=\"_blank\">https://wandb.ai/makcfd/Lr-Batch_debug/runs/t2f6d0m3</a><br/>Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230806_183028-t2f6d0m3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_run: None\n"
     ]
    }
   ],
   "source": [
    "#criterion = nn.BCELoss()\n",
    "#optimizer = torch.optim.SGD(clf.parameters(),lr=learning_rate)\n",
    "#optimizer = torch.optim.Adam(clf.parameters(),lr=learning_rate)\n",
    "#run_train(config) \n",
    "# or may be pass one path ? /exploiting_model_multiplicity/data\n",
    "\n",
    "agent_run = wandb.agent(\n",
    "    sweep_id,\n",
    "    function=run_train,\n",
    "    # q?: and project here ? what for? \n",
    "    #project=\"thesis-test\",\n",
    "    count=1,\n",
    ")\n",
    "print(\"agent_run:\", agent_run)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

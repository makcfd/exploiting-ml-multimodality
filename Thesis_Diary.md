# How to run experiment end-to-end
## Run experiment
run python script "run_experiment.py". pass config name of sweep id to start training 
python3 run_experiment.py --config test-config.yaml
## Get models 
python3 get_models.py --config test-config.yaml
## Get predictions 
python3 get_predictions.py --config test-config.yaml
## Run analysis
/my_utilities/results.py

# Files to use:
- wand_artefacts_test.ipynb: experiments with WandB
  - log artefact
  - save (link) to model registry 
- Simple_NN_WandB.ipynb: almost ready to run experiments
## Training
- NN_WandB.ipynb - training with SWIPE implemented
## Downloading 
Get_data_from_WandB.ipynb
## Analysis
example difference /SimpleModels_Adult_WandB.ipynb


# Implemenatations requirements:
- For NN data must be stored in data/<ur dataset>/ folder in test and train subfolders with names X(features) and Y(target)


# 25.06.2023
Idea: link all saved models to model registery that called: "Candidates" with certain treshold. 
Use those candidates to pull them from WandB and perform evaluation in terms of Model multiplicity metrics. 
Once have the model multiplicity metrics , link those models into appropriate model registry, i.e models that have multiplicity bhaviour or give a model registry a neme related to evaluated metrics.

# 10.07.2023
in wand_artefacts_test.ipynb section "file on fly" works. 
## (NOTE) Artefact and Model registry:
I dont have to add model to registry. The idea of registry is to have one 
place where the team can store models. 
In WandB notations storing the model in registry means "link_artifact".
So first thing is to create artefact, add model and metadata to it, log artefact and as final step link this artefact to model registry. 

 
What i tried. Run Simple_NN_WandB and get results in experiment 7 https://wandb.ai/makcfd/thesis-test/runs/ghtbrqa8?workspace=user-makcfd
What i Want is to manage the experiment name from the beginning of training process
and start experimenting with different hyper parameters. 
Save the results and after select most appropriate and save them in model registry. 

# 11.07.2023
Tasks:
- learn how to save metadata along with model artefact
  - [X] do not need. loss saved in the run along with the config
- add metrics to model: accuracy, ROC. 
  - [x] accuracy added

- [ ] optimise without checksaver the artefact saving process
Info how to retreive data from WandB:
- get (download) metrics from runs
  - https://docs.wandb.ai/guides/track/public-api-guide#read-metrics-from-a-run
  - https://docs.wandb.ai/guides/track/public-api-guide#read-specific-metrics-from-a-run
  - https://docs.wandb.ai/guides/track/public-api-guide#compare-two-runs
  - https://docs.wandb.ai/guides/track/public-api-guide#export-metrics-from-all-runs-in-a-project-to-a-csv-file
- how to retreive the models from registry/wandb
  - https://docs.wandb.ai/guides/track/public-api-guide#download-a-file-from-a-run

Additioanly read:
- https://docs.wandb.ai/guides/track/config#set-the-configuration-with-argparse

# 13.07.2023
- add f1 metric to model:
  - Status [x] 
- add swipe to current implementation:
  - Status [x]

##### Next steps:
1. Pass experiment name and number into run name. Check if the agent count number can be taken from agent. Is yes, add it to experiment name
  - Status [x]: from run name took the experiment number. The best workaround
2. Use this to get count
  - Status [x]
3. Start simple experiments of batch size
  - Status [x]

# 14.07.2023
- Think of define success:
  - metric is to get contradicted prediction ?
1. Learn how to retrive data from wandb for analysis
  - Status [x]

# 15.07.2023
- [x] Migrate from Jupyter to Python script with args marse and yaml config

# 18.07.2023
1. get models:
  - link to model registry, tag and download. 
  - input for the function: metric name and error margin in percent

# 18.07.2023


# 01.08.2023
- Fixed function to retrive candidates in Get_data_from_WandB.ipynb
- example of saving predictions and compare are in SimpleModels_Adult_WandB.ipynb

# 06.08.2023
- [x] Implemented the config style experiments
- [x] Fixed naming for saving the models
- [x] Fixed filtering runs by run name
- [x] Measure how much time it takes to run experiments

# 11.08.2023
- [x] Make predictions and save them SIMPLE

# 18.08.2023
- [x] Add argparse to pass config: pass swipe id or congif itself and type of the run

- [x] Deal with model definition so that it can be used to:
      1. run experiment 
      2. load model from WandB and make predictions
      3. Ideas: 
        3.1. Create model class and pass to it humber of neurons (https://gist.github.com/Ajasra/e32b9030e4ecfd14dcac58ef85499810)
        3.2. JIT 
          - https://pytorch.org/tutorials/beginner/saving_loading_models.html#export-load-model-in-torchscript-format
          - https://stackoverflow.com/questions/66652447/pytorch-saving-both-weights-and-model-definition
          - https://stackoverflow.com/questions/59596075/how-to-save-model-architecture-in-pytorch/75774485#75774485
        3.3. save checkpoints (https://wandb.ai/wandb/common-ml-errors/reports/How-to-Save-and-Load-Models-in-PyTorch--VmlldzozMjg0MTE)

# 29.08.2023
- [x] From Get_data_from_WandB.ipynb create script that collects models from experiment

# 30.08.2023
- [x] Save all models after each epoch

# 31.08.2023
- [x] Analise Batch sieze vs number of changes of over epochs

# 01.09.2023
- [x] Add info print if early stopping has been triggered
- [x] Implement early stopping 
- [x] Early stopping define in config (patience,, etc)
- [x] Selection of random element from dataset in dataloader

# 03.09.2023
- [x] Finish get_predictions_random with next logic:
      1. get random observation from data loader
      2. make predictions with all models
      3. save all predictions fro this observation along with ground truth 
- [x] Round up prediction probabilities to make them look shorter


# 05.09.2023
- [ ] If the same observation is unstable for LR and Batch size ? 
- [ ] Describe data preprocessing for datasets selected
- [ ] Add information about adult dataset (unbalanced?) 
- [ ] If Adult Data set is unbalanced repeat experiments on balanced? 
- [ ] Run biger scope for Batch size
- [ ] Analyse from Gender perspective
- [ ] Find additional dataset
- [ ] Get several models ready and once they are loaded, get one sample and predict appending dataframe
- [ ] Start writing the thesis- methodology and related work overview (3-5 papers)
- [ ] Can i apply fleis Kappa for output of the models. 
- [ ] Define get models/predictions config? 

# NEXT

- [ ] Get models should get more params to retrive models 
      or defaul mode should be defines (margin define, metric name, filter experiments, num of top models)
- [ ] Save models metadata in form of csv/df and add it to folder with models. 
      name it as models_metadata.csv
- [ ] Pass model definition as json (?)
- [ ] For random seed experiments save the value of initialized weights
- [ ] Run more experiments, get models and create one dataset for analysis
- [ ] Run analysis of created dataset
- [ ] Change metric to accuracy
- [ ] Save sweep id 
- [ ] Hide WandB key to env file


# ToDo:
- [ ] For downloaded models, run predictions and compare results
- [ ] Measure how much time it takes to run experiments
- [ ] While training save predictions probabilities after each epoch as artifact.
- [ ] Refactor code so that it can be run from one line each of the segments:
  - Train models for a particular experiments setup
  - retreive models 
  - make predictions 
  - run analysis 
- [ ] Apply Knn clustering for artefacts 

### =======================================
### =======================================
### Postponed
- [ ] In Simple_NN_WandB need to add automatic input layers for NN a nd hidden layers configurations
- [ ] Implement SWIPE as utility function to training. Come up with the configuration file
- [ ] Configuration takes from SWIPE add to artefact that must be logged.

# Implementation points 

Logic:
1. Prepare data. Script ->  utilities.dataprovider.py
2. Form experiments runs and save them -> experiments parameters 
  2.1. Save models 
3. Use saved models to predict data. Use same indexes to predict 
4. Save predictions in a separate files
5. Run comparison and analisys

# TODO:

- [x] Run simple models
- [x] Create simple NN
- [x] Define necessary fucntions and classes
- [x] Intergrate W and B to save result of training (WandB Upload)
  - [x] Save pkl file locally and then send it to wandb -> wandb_artefact_test.ipynb

- [ ] implement function responsible for models config with ml_collections:
``` (def set_config():
  config={
    "learning_rate": 0.01 + 0.1 * random.random(),
    "batch_size": 128,
    "architecture": "CNN",
  }
  return config
)
```

- [ ] Create prediction comparison. ! Compare models that are relatively close in performance
      Decide on error margin. 
- [ ] Optimise usage of random seed (part of experiment)
- [ ] Setup model registry on WandB
- [ ] Implement torch compile
- [ ] Run wadnb finish experiment

1. Create model loader
Use Python OrderedDict as NN model architecture to experiment with different NN models setups
Also try to use ml_collections config as blueprint for architecture

'''from collections import OrderedDict
model = nn.Sequential(OrderedDict([
                      ('fc1', nn.Linear(input_size, hidden_sizes[0])),
                      ('relu1', nn.ReLU()),
                      ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),
                      ('relu2', nn.ReLU()),
                      ('output', nn.Linear(hidden_sizes[1], output_size)),
                      ('softmax', nn.Softmax(dim=1))]))

model'''

2.         Argparse flags
        ```python
        wandb.init()
        wandb.config.epochs = 4

        parser = argparse.ArgumentParser()
        parser.add_argument(
            "-b",
            "--batch-size",
            type=int,
            default=8,
            metavar="N",
            help="input batch size for training (default: 8)",
        )
        args = parser.parse_args()
        wandb.config.update(args)
        ```


2. visualise neurons from NN to check if they "fire"
3. save trained models that performs >= to baseline and save the hyperparams with which it has been trained
4. use saved models to get predictions
5. save predictions
6. analyse predictions -> i.e evaluate prediction multiplicity 

## Useful links 

2. https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel

3. https://pytorch.org/docs/stable/notes/randomness.html

4. Thesis experiment https://pytorch.org/docs/stable/notes/randomness.html#avoiding-nondeterministic-algorithms

5. How to set random https://wandb.ai/sauravmaheshkar/RSNA-MICCAI/reports/How-to-Set-Random-Seeds-in-PyTorch-and-Tensorflow--VmlldzoxMDA2MDQy

6. https://medium.com/swlh/alpine-slim-stretch-buster-jessie-bullseye-bookworm-what-are-the-differences-in-docker-62171ed4531d

7.  https://mlinproduction.com/docker-for-ml-part-3/

8. do many changes on the model and save results of instance prediction and how it shifts 
9. do many changes on the model and save resulta on all test predicton, accumulate instability. Instability can be measured like in a paper Prediction Miltiplicity 

10. Datamodels: https://www.youtube.com/watch?v=1djD0kK2uik


11. Config usage and project example https://github.com/Gladiator07/Kaggle-Template/blob/main/src/train.py


12. NN training hints:
https://wandb.ai/site/articles/fundamentals-of-neural-networks
#### Loss
Regression: Mean squared error is the most common loss function to optimize for, unless there are a significant number of outliers. In this case, use mean absolute error or Huber loss.
Classification: Cross-entropy will serve you well in most cases.

#### Batch Size
The right weight initialization method can speed up time-to-convergence considerably. The choice of your initialization method depends on your activation function. Some things to try:
When using ReLU or leaky RELU, use He initialization
When using SELU or ELU, use LeCun initialization
When using softmax, logistic, or tanh, use Glorot initialization
Most initialization methods come in uniform and normal distribution flavors.
# Files to use:
- wand_artefacts_test.ipynb: experiments with WandB
  - log artefact
  - save (link) to model registry 
- Simple_NN_WandB.ipynb: almost ready to run experiments
  - 

# Implemenatations requirements:
- For NN data must be stored in data/<ur dataset>/ folder in test and train subfolders with names X(features) and Y(target)


# 25.06.2023
Idea: link all saved models to model registery that called: "Candidates" with certain treshold. 
Use those candidates to pull them from WandB and perform evaluation in terms of Model multiplicity metrics. 
Once have the model multiplicity metrics , link those models into appropriate model registry, i.e models that have multiplicity bhaviour or give a model registry a neme related to evaluated metrics.

# 10.07.2023
in wand_artefacts_test.ipynb section "file on fly" works. 
## (NOTE) Artefact and Model registry:
I dont have to add model to registry. The idea of registry is to have one 
place where the team can store models. 
In WandB notations storing the model in registry means "link_artifact".
So first thing is to create artefact, add model and metadata to it, log artefact and as final step link this artefact to model registry. 

 
What i tried. Run Simple_NN_WandB and get results in experiment 7 https://wandb.ai/makcfd/thesis-test/runs/ghtbrqa8?workspace=user-makcfd
What i Want is to manage the experiment name from the beginning of training process
and start experimenting with different hyper parameters. 
Save the results and after select most appropriate and save them in model registry. 

# 11.07.2023
Tasks:
- learn how to save metadata along with model artefact
  - [X] do not need. loss saved in the run along with the config
- add metrics to model: accuracy, ROC. 
  - [x] accuracy added

- [ ] optimise without checksaver the artefact saving process
Info how to retreive data from WandB:
- get (download) metrics from runs
  - https://docs.wandb.ai/guides/track/public-api-guide#read-metrics-from-a-run
  - https://docs.wandb.ai/guides/track/public-api-guide#read-specific-metrics-from-a-run
  - https://docs.wandb.ai/guides/track/public-api-guide#compare-two-runs
  - https://docs.wandb.ai/guides/track/public-api-guide#export-metrics-from-all-runs-in-a-project-to-a-csv-file
- how to retreive the models from registry/wandb
  - https://docs.wandb.ai/guides/track/public-api-guide#download-a-file-from-a-run

Additioanly read:
- https://docs.wandb.ai/guides/track/config#set-the-configuration-with-argparse

# 13.07.2023
- add f1 metric to model:
  - Status [x] 
- add swipe to current implementation:
  - Status [x]

##### Next steps:
1. Pass experiment name and number into run name. Check if the agent count number can be taken from agent. Is yes, add it to experiment name
  - Status [x]: from run name took the experiment number. The best workaround
2. Use this to get count
  - Status [x]
3. Start simple experiments of batch size
  - Status [x]

# 14.07.2023
- Think of define success:
  - metric is to get contradicted prediction ?
1. Learn how to retrive data from wandb for analysis
  - Status [ ]

# 15.07.2023
- [ ] Migrate from Jupyter to Python script with args marse and yaml config

# 18.07.2023
1. get models:
  - link to model registry, tag and download. 
  - input for the function: metric name and error margin in percent

# 18.07.2023
- [ ] Apply Knn clustering for artefacts 

### =======================================
### =======================================
### Postponed
- [ ] In Simple_NN_WandB need to add automatic input layers for NN a nd hidden layers configurations
- [ ] Implement SWIPE as utility function to training. Come up with the configuration file
- [ ] Configuration takes from SWIPE add to artefact that must be logged.

# Implementation points 

Logic:
1. Prepare data. Script ->  utilities.dataprovider.py
2. Form experiments runs and save them -> experiments parameters 
  2.1. Save models 
3. Use saved models to predict data. Use same indexes to predict 
4. Save predictions in a separate files
5. Run comparison and analisys

# TODO:

- [x] Run simple models
- [x] Create simple NN
- [x] Define necessary fucntions and classes
- [x] Intergrate W and B to save result of training (WandB Upload)
  - [x] Save pkl file locally and then send it to wandb -> wandb_artefact_test.ipynb

- [ ] implement function responsible for models config with ml_collections:
``` (def set_config():
  config={
    "learning_rate": 0.01 + 0.1 * random.random(),
    "batch_size": 128,
    "architecture": "CNN",
  }
  return config
)
```

- [ ] Create prediction comparison. ! Compare models that are relatively close in performance
      Decide on error margin. 
- [ ] Apply "W&B SWIPE" to simple NN on Adults dataset to find out the best model close to baseline
- [ ] Implement metrics from Paper 
- [ ] Create model loader. See details below
- [ ] Optimise usage of random seed (part of experiment)
- [ ] Setup model registry on WandB
- [ ] Download models from WandB registry (WandB Download)
- [ ] Move to scripts with Argparse to run experiments on server
- [ ] Implement torch compile
- [ ] Run wandb init for each experimtn, give it a name as aexperiment 
- [ ] Run wadnb finish experiment


1. Create model loader
Use Python OrderedDict as NN model architecture to experiment with different NN models setups
Also try to use ml_collections config as blueprint for architecture

'''from collections import OrderedDict
model = nn.Sequential(OrderedDict([
                      ('fc1', nn.Linear(input_size, hidden_sizes[0])),
                      ('relu1', nn.ReLU()),
                      ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),
                      ('relu2', nn.ReLU()),
                      ('output', nn.Linear(hidden_sizes[1], output_size)),
                      ('softmax', nn.Softmax(dim=1))]))

model'''

2.         Argparse flags
        ```python
        wandb.init()
        wandb.config.epochs = 4

        parser = argparse.ArgumentParser()
        parser.add_argument(
            "-b",
            "--batch-size",
            type=int,
            default=8,
            metavar="N",
            help="input batch size for training (default: 8)",
        )
        args = parser.parse_args()
        wandb.config.update(args)
        ```


2. visualise neurons from NN to check if they "fire"
3. save trained models that performs >= to baseline and save the hyperparams with which it has been trained
4. use saved models to get predictions
5. save predictions
6. analyse predictions -> i.e evaluate prediction multiplicity 

## Useful links 

2. https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel

3. https://pytorch.org/docs/stable/notes/randomness.html

4. Thesis experiment https://pytorch.org/docs/stable/notes/randomness.html#avoiding-nondeterministic-algorithms

5. How to set random https://wandb.ai/sauravmaheshkar/RSNA-MICCAI/reports/How-to-Set-Random-Seeds-in-PyTorch-and-Tensorflow--VmlldzoxMDA2MDQy

6. https://medium.com/swlh/alpine-slim-stretch-buster-jessie-bullseye-bookworm-what-are-the-differences-in-docker-62171ed4531d

7.  https://mlinproduction.com/docker-for-ml-part-3/

8. do many changes on the model and save results of instance prediction and how it shifts 
9. do many changes on the model and save resulta on all test predicton, accumulate instability. Instability can be measured like in a paper Prediction Miltiplicity 

10. Datamodels: https://www.youtube.com/watch?v=1djD0kK2uik


11. Config usage and project example https://github.com/Gladiator07/Kaggle-Template/blob/main/src/train.py


12. NN training hints:
https://wandb.ai/site/articles/fundamentals-of-neural-networks
#### Loss
Regression: Mean squared error is the most common loss function to optimize for, unless there are a significant number of outliers. In this case, use mean absolute error or Huber loss.
Classification: Cross-entropy will serve you well in most cases.

#### Batch Size
The right weight initialization method can speed up time-to-convergence considerably. The choice of your initialization method depends on your activation function. Some things to try:
When using ReLU or leaky RELU, use He initialization
When using SELU or ELU, use LeCun initialization
When using softmax, logistic, or tanh, use Glorot initialization
Most initialization methods come in uniform and normal distribution flavors.